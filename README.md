<br />
<div align="center">

  <h3 align="center">Sistemas Distribuidos: Tarea 03</h3>

  <p align="center">
    Fernando Bur√≥n, Felipe Condore
  </p>
 
</div>

## Acerca del proyecto

El objetivo de esta tarea consiste en poner en pr√°ctica el concepto de replicaci√≥n en bases de datos utilizando Cassandra


### üõ† Constru√≠do con:

Esta secci√≥n muestra las tecnolog√≠as con las que fue constru√≠do el proyecto.

* [Apache Cassandra](https://cassandra.apache.org/_/index.html)
* [Python](https://www.python.org)
* [Docker](https://www.docker.com)


## :octocat: Comenzando

Para iniciar el proyecto, primero hay que copiar el repositorio `git clone https://github.com/CondePoponcio/SD-T3-cassandra .` y escribir el siguiente comando en la consola:
* docker
```sh
docker-compose build
```
Para que los contenedores se inician en el ambiente local se utiliza el siguiente comando en la consola:
* docker
```sh
docker-compose up -d
```
Los tiempos de iniciaci√≥n son extensos debido a la alta demanda de c√≥mputo para los nodos de Cassandra

### Pre-Requisitos

Tener Docker y Docker Compose instalado
* [Installation Guide](https://docs.docker.com/compose/install/)


## ü§ù Uso

La aplicaci√≥n tiene una API disponible en el puerto 3000 :trollface:

### Create
Genera una receta para un paciente.
#### Request example
```sh
curl --location --request POST http://localhost:3000/create \
--header 'Content-Type: application/json' \
--data-raw '{
    "nombre": "Cosme", 
    "apellido": "Fulanito",
    "rut": "19650228-9",
    "email": "fcondore@gmail.com",
    "fecha_nacimiento": "1998-08-28",
    "comentario": "Nada en particular",
    "farmacos": "Su paracetamil",
    "doctor": "Dr. Simi"
}'
```
#### Response example
```json
{
  "status":"success"
}
```

### Edit
Edita una receta en espec√≠fico
#### Request example
```sh
curl --location --request POST http://localhost:3000/edit \
--header 'Content-Type: application/json' \
--data-raw '{
    "id": "4483b1fa-cfd6-4351-952f-87f4eff8c9bd",
    "comentario": "Nada en particular XD",
    "farmacos": "Drogas de la buena",
    "doctor": "Jordi"
}'
```
#### Response example
```json
{
  "status":"success"
}
```

### Delete
Elimina una receta
#### Request example
```sh
curl --location --request POST http://localhost:3000/delete \
--header 'Content-Type: application/json' \
--data-raw '{
    "id": "4483b1fa-cfd6-4351-952f-87f4eff8c9bd",
}'
```
#### Response example
```json
{
  "status":"success"
}
```
## ‚ùî Preguntas

### 1. Explique la arquitectura que Cassandra maneja al crear un cl√∫ster:
* **¬øC√≥mo los nodos se conectan?** La arquitectura de Cassandra est√° creada teniendo en cuenta que el sistema puede fallar en alg√∫n punto, por lo que su conjunto est√° conformado de Peers que se conectan unos a los otros (P2P). Los datos son distribuidos a lo largo de los nodos del cl√∫ster y estos se conectan a trav√©s de _Internode communications (gossip)_. Gossip es un protocolo de conexi√≥n Peer-to-peer, en donde cada uno de los nodos comparten su estado peri√≥dicamente, su informaci√≥n y tambi√©n la que tienen de otros nodos. Estas actualizaciones ocurren cada segundo, lo que permite que cada nodo aprenda r√°pidamente del resto. Un mensaje _gossip_, en su estructura, cuenta con una versi√≥n, lo que admite sobreescribir mensajes que est√°n a destiempo. Adicionalmente, al iniciar un cl√∫ster se define un nodo semilla, que ser√° el primero en en iniciar este procesos de "chismes" (_gossip_), esto no significa que haya un √∫nico punto de falla, ya que luego de ser inicializado, todos los nodos tienen la opci√≥n de entregar y recibir _gossip messages_. Adem√°s, cada nodo est√° conectado con dos nodos vecinos, formando as√≠ una arquitectura tipo anillo, haciendo que los datos viajen en una ruta continua. Esto est√° pensado as√≠ para que la base de datos NoSQL pueda escalar de manera indefinida.
* **¬øQu√© ocurre cuando un cliente realiza una petici√≥n a uno de los nodos?** El nodo que recibe la petici√≥n ser√° el coordinador, se encarga de direccionar esta petici√≥n al resto de nodos, generando distintos logs del commit realizado. Usando el protocolo _gossip_ los mensajes se distribuyen en cada una de las r√©plicas y se dirigen a la MemTable, almacen√°ndolos en el volumen que SSTable.
* ¬øQu√© ocurre cuando uno de los nodos se desconecta? Cuando uno de los nodos se desconecta y suponiendo que hay pocos, existen distintas estrategias que Cassandra toma para tolerar este problema: Snitch y SimpleStrategy. El primero le ense√±a lo suficiente a Cassandra acerca de la topolog√≠a de su red para poder rutear correctamente las peticiones, adem√°s de distribuir r√©plicas alrededor del cl√∫ster para evitar fallas correlacionadas, espec√≠ficamente agrupa las m√°quinas en _datacenters_ y _racks_, Cassandra intentar√° que no exista m√°s de una r√©plica en el mismo _rack_ (puede no ser una ubicaci√≥n f√≠sica). El segundo es usado cuando tienes un solo _datacenter_, SimpleStrategy coloca una primera r√©plica en el nodo seleccionado por el particionador, luego de eso las r√©plicas restantes son colocadas en sentido del reloj en el anillo de Nodos. Adem√°s, todas las desconexiones de nodos son avisadas a trav√©s de Gossip al resto. Adem√°s el balanceador de cargas determinar√° qu√© nodos se usar√°n en caso de fallo
* **¬øLa red generada entre los nodos siempre es eficiente?** No siempre, depender√° de la magnitud de datos que se est√°n procesando, al estar pensada para escalar masivamente. Si se utiliza para muchos datos, pero el c√≥mputo de los nodos no es lo suficiente para poder procesasrlos, puede generarse un cuello de botella. Adem√°s, una red P2P tiene limitaciones, la latencia puede aumentar por cada nodo presentado, no hay que olvidar que la comunicaci√≥n por el protocolo _gossip_ pasa por cada uno de los nodos y, si hay demasiados, la latencia es evidente, lo que concluye en ineficiencia en ciertos casos.
* **¬øExiste balanceo de carga?** S√≠, Cassandra cuenta con un sistema LBP (_load balancing policy_), es un componente que determina qu√© nodos el driver se comunicar√°n y, por cada una de las nuevas querys, cu√°l coordinador escoger y qu√© nodos usar en casos de un fallo.
<br />
<div align="center">
<image src=https://i.imgur.com/MLdMopS.png>
 </div>
 
### 2. Cassandra posee principalmente dos estrategias para mantener redundancia en la replicaci√≥n de datos:
* **¬øCu√°les son?** SimpleStrategy y NetworkTopologyStrategy
* **¬øCu√°l es la ventaja de una sobre la otra?** SimpleStrategy es usada cuando s√≥lo hay un _datacenter_, coloca una primera r√©plica en el nodo seleccionado por el particionador, luego de eso las r√©plicas restantes son colocadas en sentido del reloj en el anillo de Nodos. Tiene ventaja en sistemas de tama√±o peque√±o. Por otro lado, NetworkToplogyStrategy sirve cuando se tiene m√°s de un _datacenter_, lo que implica que escalar√° mejor en el futuro, teniendo la capacidad de a√±adir nuevas regiones de datos, se usa en la mayor√≠a de despliegues. 
* **¬øCu√°l utilizar√≠a usted en el caso actual y por qu√©?** En este caso, al ser una aplicaci√≥n simple y de baja escala, se utilizar√≠a SimpleStrategy. 

### 3. Teniendo en cuenta el contexto del problema: 
_Oriente su respuesta hacia el Sharding (la replicaci√≥n/distribuci√≥n de los datos) y comente una estrategia que podr√≠a seguir para ordenar los datos._
* **¬øUsted cree que la soluci√≥n propuesta es la correcta?** Depende, se debe tener en consideraci√≥n la cantidad de peticiones, usuarios concurrentes y el tama√±o de los datos a procesar. Actualmente la topolog√≠a utilizada es solo un _datacenter_, lo que no permite crear nuevas regiones de datos y en t√©rminos de escalabilidad puede no ser la mejor opci√≥n. Considerando un escenario real, donde existen miles de peticiones por segundo, se podr√≠a crear una topolog√≠a de _datacenters_ y _racks_, pero esto requerir√≠a una mayor inversi√≥n en el _hardware_ y en el _software_. 
* **¬øQu√© ocurre cuando se quiere escalar en la soluci√≥n?** La estrateg√≠a SimpleStrategy no es la mejor opci√≥n, por lo tanto habr√≠a que considerar la topolog√≠a de _datacenters_ y _racks_. Teniendo en cuenta la cantidad de datos que podr√≠an ser manejados en el sistema (recetas, pacientes, etc.), lo mejor ser√≠a realizar un escalamiento horizontal con un sistema de _Sharding_. El _Sharding_ permite la partici√≥n horizontal de los datos, donde cada partici√≥n individual se denomina _shard_, lo que permite extender la carga. 
A continuaci√≥n, se muestra una imagen de una arquitectura que incluye _Sharding_ en un escenario donde hay dos regiones (AZ-1 y AZ-2).
<br />
<div align="center">
<image src=/img/Sharding.drawio.png>
 </div>
* **¬øQu√© mejoras implementar√≠a?**



## ‚Ñπ Informaci√≥n Importante
El uso de las im√°genes de [Bitnami](https://hub.docker.com/u/bitnami) fueron reemplazadas por [wurstmeister](https://hub.docker.com/u/wurstmeister) por el simple hecho de que la utilizaci√≥n de [AIOKafka](https://github.com/aio-libs/aiokafka) no permit√≠a establecer una conexi√≥n con el contenedor de Kafka. Esta librer√≠a de Python permite utilizar Kafka de manera asincr√≥nica, exactamente lo que se requer√≠a para combinar Flask con un KafkaProducer. Para el api de bloqueo se us√≥ un KafkaProducer as√≠ncrono basado en la contribuci√≥n de trabajo de [NimzyMaina](https://github.com/NimzyMaina/flask_kafka).
